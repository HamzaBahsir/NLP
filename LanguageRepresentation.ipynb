{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamzaBahsir/NLP/blob/main/LanguageRepresentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mea-CemjMt15"
      },
      "source": [
        "# **LAB 3: Language Representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTCiwRXMdlVS"
      },
      "source": [
        "**Language Representation** a.k.a. Text Representation is the process of converting unstructured text data into a structured format (machine-readable form). It involves converting words, phrases, or entire documents into numerical or symbolic representations while preserving meaning and context.\n",
        "\n",
        "It comprise preprocessing the text data followed by selecting a suitable representation scheme, such as Bag-of-Words, TF-IDF etc. to capture the key features and characteristics of the same, in a numerical form that can be processed by machine learning algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUPIYDoTM3yo"
      },
      "source": [
        "# **Objectives:**\n",
        "Here we will do\n",
        "\n",
        "1. Text Preprocessing\n",
        "    * Remove Punctuation\n",
        "    * Remove URLs\n",
        "    * Lowercasing\n",
        "    * Tokenization\n",
        "    * Remove Stop Words\n",
        "    * Stemming\n",
        "    * Lemmatization\n",
        "2. Character Encoding\n",
        "    * ASCII\n",
        "    * UTF-8\n",
        "3. Text Representation\n",
        "    * Bag-of-Words (BoW)\n",
        "    * Term Frequency - Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Extra Resources**\n",
        "[Natural Language Processing with Python](https://www.nltk.org/book/)"
      ],
      "metadata": {
        "id": "2WM0MmzqBWvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Libraries Required**\n",
        "1.   nltk\n",
        "2.   string\n",
        "3.   re\n",
        "4.   sklearn\n",
        "\n"
      ],
      "metadata": {
        "id": "_IFoXNNnBaQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "#### For Removing Punctuation\n",
        "import string\n",
        "#### For Removing URLs\n",
        "import re\n",
        "#### For Tokenization\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "#### For Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "#### For Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "#### For BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#### For TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7LvbvWBopZ",
        "outputId": "e7623f94-11f3-43c4-aa92-c93656cdde58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  **Text Preprocessing**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ogTyWzhdWAdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Punctuation**"
      ],
      "metadata": {
        "id": "7Z_70YsO2c3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world! NLP is amazing. Let's learn it at https://example.com.\"\n",
        "text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(text_no_punct)  #output:(Hello world NLP is amazing Lets learn it at httpsexamplecom)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXJT-gNIY2Nt",
        "outputId": "cc36d307-a1d0-4ee0-9587-46f11d7f654a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world NLP is amazing Lets learn it at httpsexamplecom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove URLs**"
      ],
      "metadata": {
        "id": "uySpPCrt2hUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text is text_no_punct computed above\n",
        "text_no_urls = re.sub(r'http\\S+|www\\S+', '', text_no_punct)\n",
        "# print the output\n",
        "print(text_no_urls)   #output:(Hello world NLP is amazing Lets learn it at)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbiZcvsla4N0",
        "outputId": "80448a67-43d3-439e-a676-63da0a292463"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world NLP is amazing Lets learn it at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lowercasing**"
      ],
      "metadata": {
        "id": "n-J0Xp6l2yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text is \"text_no_urls\" computed above\n",
        "text_lower = text_no_urls.lower()\n",
        "# print the output\n",
        "print(text_lower)     # Output:(hello world nlp is amazing lets learn it at)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVaPSq1biHL",
        "outputId": "ab3b3707-be3f-4757-8d44-9f8581b52ee4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world nlp is amazing lets learn it at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**"
      ],
      "metadata": {
        "id": "8SuBpf0D6i73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text is \"text_lower\" computed above\n",
        "words = word_tokenize(text_lower)\n",
        "sentences = sent_tokenize(text_lower)\n",
        "\n",
        "# print both outputs\n",
        "print(sentences)      # Output:['hello world nlp is amazing lets learn it at']\n",
        "print(words)          # Output:['hello', 'world', 'nlp', 'is', 'amazing', 'lets',\n",
        "                      # 'learn', 'it', 'at']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InXXv3ia8egR",
        "outputId": "6e904419-eb36-4bc0-f229-7421b7d5335d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello world nlp is amazing lets learn it at']\n",
            "['hello', 'world', 'nlp', 'is', 'amazing', 'lets', 'learn', 'it', 'at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Stop Words**"
      ],
      "metadata": {
        "id": "wk0AINHp2rKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the `words` from previous block\n",
        "filtered_text = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "# print the output\n",
        "print(filtered_text)  # Output:['hello', 'world', 'nlp', 'amazing', 'lets', 'learn']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDR-VcehbfOR",
        "outputId": "254cdc1c-7c18-4d96-9702-cd494a0bb92a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'nlp', 'amazing', 'lets', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "IciaoQfU28AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the `filtered_text` from previous block\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_text]\n",
        "# print the output\n",
        "print(stemmed_words)  # Output:['hello', 'world', 'nlp', 'amaz', 'let', 'learn']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtfw5foQ9xWG",
        "outputId": "ef72eb83-dacb-47f6-fc19-075caa715608"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'nlp', 'amaz', 'let', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "aIB6tNBK2-xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the `filtered_text` from previous block\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"n\") for word in filtered_text]  # 'n' for noun\n",
        "# print the output\n",
        "print(lemmatized_words)# Output:['hello', 'world', 'nlp', 'amazing', 'let', 'learn']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkDPa4cU9X5A",
        "outputId": "d4bd3b21-b10f-417b-f0b0-d83e6d6644f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'nlp', 'amazing', 'let', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Valid options for `pos` in `.lemmatize()` are “n” for nouns, “v” for verbs, “a” for adjectives, “r” for adverbs and “s” for satellite adjectives.\n",
        "\n"
      ],
      "metadata": {
        "id": "12h10fYe_w6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  **Character Encoding**"
      ],
      "metadata": {
        "id": "HlyctAVVY3fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **American Standard Code for Information Interchange (ASCII)**\n"
      ],
      "metadata": {
        "id": "_QwuLFRaEoUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, دنیا\"\n",
        "\n",
        "# Encode using ASCII (ignoring non-ASCII characters)\n",
        "encoded_text = text.encode(\"ascii\", errors=\"ignore\")\n",
        "print(\"Encoded Text: \", encoded_text)  # Output: b'Caf'\n",
        "\n",
        "# Encode using ASCII (replacing non-ASCII characters)\n",
        "decoded_text = encoded_text.decode(\"ascii\")\n",
        "print(\"Decoded Text: \", decoded_text)  # Output: Hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u79AyCo6xt7u",
        "outputId": "8fccfe11-4523-4c9d-fa18-0ce2fdca5188"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'Hello, '\n",
            "Decoded Text:  Hello, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Unicode Transformation Format 8 (UTF-8)**\n"
      ],
      "metadata": {
        "id": "YzEPaHWhEnY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ہیلو، دنیا\"  # Unicode string in Urdu\n",
        "\n",
        "############ Encode to bytes using UTF-8\n",
        "# Encode using UTF-8\n",
        "encoded_text = text.encode(\"utf-8\", errors=\"ignore\")\n",
        "# print the output\n",
        "print(\"Encoded Text: \", encoded_text)  #Output:(b'\\xdb\\x81\\xdb\\x8c\\xd9\\x84\\xd9\\x88\\xd8\\x8c \\xd8\\xaf\\xd9\\x86\\xdb\\x8c\\xd8\\xa7')\n",
        "\n",
        "############ Decode back to string using .decode()\n",
        "decoded_text = encoded_text.decode(\"utf-8\")\n",
        "# print the output\n",
        "print(\"Decoded Text: \", decoded_text)  #Output:(ہیلو، دنیا)"
      ],
      "metadata": {
        "id": "tdfclDuUyJ2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6d5cba-601f-4b3b-a3dc-512d38f2ca43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'\\xdb\\x81\\xdb\\x8c\\xd9\\x84\\xd9\\x88\\xd8\\x8c \\xd8\\xaf\\xd9\\x86\\xdb\\x8c\\xd8\\xa7'\n",
            "Decoded Text:  ہیلو، دنیا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Text Representation**\n",
        "\n",
        "a) **Bag-of-Words (BoW) Representation:**\n",
        "\n",
        "It represents text as a vector of word frequencies, ignoring grammar and word order, based on a corpus-wide vocabulary.\n",
        "\n",
        "\n",
        "b) **Term Frequency - Inverse Document Frequency (TF-IDF) Representation:**\n",
        "\n",
        "It is a statistical measure that evaluates a word's importance in a document relative to a collection of documents by combining its frequency in the document (TF) and its rarity across the corpus (IDF).\n",
        "\n",
        "Words that appear frequently across many documents (common words) have lower importance."
      ],
      "metadata": {
        "id": "ygOZDTxNf_tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BoW**"
      ],
      "metadata": {
        "id": "tgXiBAngRF_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input texts\n",
        "text1 = \"I love NLP.\"\n",
        "text2 = \"NLP is an interesting subject.\"\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "# Initialize the CountVectorizer, which converts text into a matrix of token counts\n",
        "bow_vectorizer = CountVectorizer()\n",
        "# Fit and transform the input texts into a BoW matrix\n",
        "bow_matrix = bow_vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Feature names and BoW representation\n",
        "print(\"Bag of Words (BoW):\")\n",
        "print(\"Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRtvl7UYJyUA",
        "outputId": "dce9fa17-4d55-4276-f84f-53b8f924b790"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW):\n",
            "Feature Names: ['an' 'interesting' 'is' 'love' 'nlp' 'subject']\n",
            "BoW Matrix:\n",
            " [[0 0 0 1 1 0]\n",
            " [1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: `vectorizer.fit_transform()` build a unique vocabulary by\n",
        "  * Applying Tokenization\n",
        "  * Removing Duplicates\n",
        "  * Lowercasing\n",
        "  * Stop Word Removal"
      ],
      "metadata": {
        "id": "wWeBbQGBOhAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**"
      ],
      "metadata": {
        "id": "5rZRySOfQ2ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "# Same text to be used as BoW\n",
        "# Input texts\n",
        "text1 = \"I love NLP.\"\n",
        "text2 = \"NLP is an interesting subject.\"\n",
        "\n",
        "# Initialize the TfidfVectorizer, which transforms text into TF-IDF\n",
        "TfIdf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the input texts into a TF-IDF matrix\n",
        "TfIdf_matrix = TfIdf_vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Print the Feature names and TF-IDF representation\n",
        "print(\"Term Frequency -- Inverse Document Frequency (TF-IDF):\")\n",
        "print(\"Feature Names:\", TfIdf_vectorizer.get_feature_names_out())\n",
        "print(\"TfIdf Matrix:\\n\", TfIdf_matrix.toarray())\n",
        "\n"
      ],
      "metadata": {
        "id": "9qR5T6Dszsct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e947ef-81c1-44b1-e2fb-fa3b73537668"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency -- Inverse Document Frequency (TF-IDF):\n",
            "Feature Names: ['an' 'interesting' 'is' 'love' 'nlp' 'subject']\n",
            "TfIdf Matrix:\n",
            " [[0.         0.         0.         0.81480247 0.57973867 0.        ]\n",
            " [0.47107781 0.47107781 0.47107781 0.         0.33517574 0.47107781]]\n"
          ]
        }
      ]
    }
  ]
}